# how to run orion when multiple normal runs are available

$ python2 ./orion.py -n data/hadoop_case/normal_med.dat,data/hadoop_case/normal1.dat,data/hadoop_case/normal2.dat -a data/hadoop_case/abnormal_med.dat --select-metrics

$ ./orion.py -n data/hadoop_case/normal_med.dat -a data/hadoop_case/abnormal_med.dat --select-metrics
Localization module loaded.
Correlation analysis module loaded.
[ORION]: Normal File: data/hadoop_case/normal_med.dat
[ORION]: Abnormal File: data/hadoop_case/abnormal_med.dat
[ORION]: Loading data files...
[ORION]: Calculating correlations for window-size: 100
[ORION]: Finding abnormal correlations...
[ORION]: Calculating correlations for window-size: 125
[ORION]: Finding abnormal correlations...
[ORION]: Calculating correlations for window-size: 150
[ORION]: Finding abnormal correlations...
[ORION]: Calculating correlations for window-size: 175
[ORION]: Finding abnormal correlations...
[ORION]: Calculating correlations for window-size: 200
[ORION]: Finding abnormal correlations...

========== Top-3 Abnormal Metrics ==========
Format: [Rank] [Metric]
[1]: rss
[2]: num_file_desc
[3]: minflt

========== Other Metrics ==========
[4]: write_bytes
[5]: num_threads
[6]: vsize
[7]: wchar
[8]: stime
[9]: rchar
[10]: utime


$ ./orion.py -n data/hadoop_case/normal_med.dat -a data/hadoop_case/abnormal_med.dat --select-regions -m num_file_desc
Localization module loaded.
Correlation analysis module loaded.
[ORION]: Normal File: data/hadoop_case/normal_med.dat
[ORION]: Abnormal File: data/hadoop_case/abnormal_med.dat
[ORION]: Finding outliers...

========== Top-3 Abnormal Code Regions ==========
[1]:
	[218] org/apache/hadoop/dfs/DFSClient
[2]:
	[183] org/apache/hadoop/dfs/BlocksMap
[3]:
	[172] org/apache/hadoop/dfs/DataNode


$ ./orion.py -n data/hadoop_case/normal_med.dat -a data/hadoop_case/abnormal_med.dat --select-classname -c DFSClient -p 0

Localization module loaded.
Correlation analysis module loaded.
[ORION]: Normal File: data/hadoop_case/normal_med.dat
[ORION]: Abnormal File: data/hadoop_case/abnormal_med.dat

dummy
org/apache/hadoop/dfs/DFSClient$LeaseChecker$run
org/apache/hadoop/dfs/DFSClient$datanodeReport
org/apache/hadoop/dfs/DFSClient$LeaseChecker$run
org/apache/hadoop/dfs/DFSClient$datanodeReport
org/apache/hadoop/dfs/DFSClient$LeaseChecker$run
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$writeChunk
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$DataStreamer$run
org/apache/hadoop/dfs/DFSClient$access$500
org/apache/hadoop/dfs/DFSClient$checkOpen
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$isClosed
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$close
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$closeInternal
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$flushInternal
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1700
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$ResponseProcessor$run
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1000
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1200
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1000
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1000
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1600
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$closeThreads
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$DataStreamer$close
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1000
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1000
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$DataStreamer$run
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$writeChunk
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1000
org/apache/hadoop/dfs/DFSClient$access$500
org/apache/hadoop/dfs/DFSClient$checkOpen
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1100
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$close
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$closeInternal
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$flushInternal
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1700
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$ResponseProcessor$run
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1600
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1900
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$800
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1600
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$closeThreads
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$DataStreamer$close
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1800
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1800
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1000
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$DataStreamer$run
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$writeChunk
org/apache/hadoop/dfs/DFSClient$access$500
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$writeChunk
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$isClosed

org/apache/hadoop/dfs/DFSClient$LeaseChecker$run
org/apache/hadoop/dfs/DFSClient$datanodeReport
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1000
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$DataStreamer$run
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$writeChunk
org/apache/hadoop/dfs/DFSClient$access$500
org/apache/hadoop/dfs/DFSClient$checkOpen
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$close
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$closeInternal
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$flushInternal
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1700
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$ResponseProcessor$run
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1600
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$closeThreads
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$DataStreamer$close
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1800
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$isClosed
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1100
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1900
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$800
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$900
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$ResponseProcessor$close
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1400
org/apache/hadoop/dfs/DFSClient$create
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$processDatanodeError
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$1200
org/apache/hadoop/dfs/DFSClient$DFSOutputStream$access$902

dummy
org/apache/hadoop/dfs/DFSClient$createRPCNamenode
